{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9581f393-b12b-4975-bf5d-6c0816aeea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  **scheduler_specific_kwargs,\n",
      "Training Epoch 1/3: 100%|█████████████████████████| 2/2 [00:06<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 14.265144348144531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|█████████████████████████| 2/2 [00:06<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 9.34230375289917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|█████████████████████████| 2/2 [00:06<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 6.929811954498291\n"
     ]
    }
   ],
   "source": [
    "#model (debiased)\n",
    "#finetuning + debiasing integrated\n",
    "#it debias all tokens\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#load and norm. warmth/competence vectors\n",
    "warmth_vector = pd.read_csv('/Users/aleksandragarbat/Desktop/Thesis/warmth_direction.csv').values.flatten()\n",
    "competence_vector = pd.read_csv('/Users/aleksandragarbat/Desktop/Thesis/competence_direction.csv').values.flatten()\n",
    "\n",
    "warmth_vector = torch.tensor(warmth_vector, dtype=torch.float32)\n",
    "competence_vector = torch.tensor(competence_vector, dtype=torch.float32)\n",
    "\n",
    "warmth_vector = warmth_vector / warmth_vector.norm()\n",
    "competence_vector = competence_vector / competence_vector.norm()\n",
    "\n",
    "#bias subspace setup\n",
    "def gram_schmidt(vectors):\n",
    "    ortho = []\n",
    "    for v in vectors:\n",
    "        for u in ortho:\n",
    "            v = v - torch.dot(v, u) * u\n",
    "        v = v / torch.norm(v)\n",
    "        ortho.append(v)\n",
    "    return ortho\n",
    "\n",
    "g0, g1 = gram_schmidt([warmth_vector, competence_vector])\n",
    "components = [g0, g1]\n",
    "weights = [0.6, 0.4] #there weights of either w or c can be adjusted and accounted for in debiasing\n",
    "\n",
    "#debiasing function\n",
    "def debias_custom(h, components, weights, n_mask=None):\n",
    "    debiased = h.clone()\n",
    "    for i, g in enumerate(components):\n",
    "        weight = weights[i]\n",
    "        n = n_mask[i] if n_mask is not None else 1\n",
    "        debiased -= weight * n * torch.dot(h, g) * g\n",
    "    return debiased\n",
    "\n",
    "#custom dataset for MLM (this sample was created based on descriptions from O'net job descriptins)\n",
    "class MLM_Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].flatten()\n",
    "        attention_mask = encoding[\"attention_mask\"].flatten()\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids\n",
    "        }\n",
    "\n",
    "#updating BERT model with optional full-sequence debiasing\n",
    "class DebiasedBertForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config, components, weights, tokenizer, debias_all_tokens=True):\n",
    "        super().__init__(config)\n",
    "        self.components = components\n",
    "        self.weights = weights\n",
    "        self.tokenizer = tokenizer\n",
    "        self.debias_all_tokens = debias_all_tokens\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        debiased_sequence_output = sequence_output.clone()\n",
    "\n",
    "        if self.debias_all_tokens:\n",
    "            # Debias all tokens\n",
    "            for batch_idx in range(input_ids.size(0)):\n",
    "                for token_idx in range(input_ids.size(1)):\n",
    "                    h = sequence_output[batch_idx, token_idx]\n",
    "                    debiased_sequence_output[batch_idx, token_idx] = debias_custom(h, self.components, self.weights)\n",
    "        else:\n",
    "            #only debias [mask] tokens (not needed)\n",
    "            mask_token_index = (input_ids == self.tokenizer.mask_token_id)\n",
    "            for batch_idx in range(input_ids.size(0)):\n",
    "                for token_idx in torch.where(mask_token_index[batch_idx])[0]:\n",
    "                    h = sequence_output[batch_idx, token_idx]\n",
    "                    debiased_sequence_output[batch_idx, token_idx] = debias_custom(h, self.components, self.weights)\n",
    "\n",
    "        prediction_scores = self.cls(debiased_sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": prediction_scores}\n",
    "\n",
    "#fine-tuning loop\n",
    "def train_finetune(model, train_dataset, tokenizer, epochs=3, batch_size=8, learning_rate=5e-5):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "# === Setup tokenizer and model ===\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "debias_model = DebiasedBertForMaskedLM.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    components=components, \n",
    "    weights=weights,\n",
    "    tokenizer=tokenizer,\n",
    "    debias_all_tokens=True \n",
    ")\n",
    "\n",
    "#sample training texts with masks \n",
    "train_texts = [\n",
    "    \"[MASK] conduct subsurface surveys to identify the characteristics of potential land or mining development sites.\",\n",
    "    \"[MASK] conduct research on nuclear engineering projects or apply principles and theory of nuclear science.\",\n",
    "    \"[MASK] devise methods to improve oil and gas extraction and production.\",\n",
    "    \"[MASK] all engineers not listed separately.\",\n",
    "    \"[MASK] design, develop, or evaluate energy-related projects or programs.\",\n",
    "    \"[MASK] research, design, develop, or test automation, intelligent systems, smart devices, or industrial systems.\",\n",
    "    \"[MASK] research, design, develop, or test microelectromechanical systems.\",\n",
    "    \"[MASK] design technologies specializing in light information or light energy.\",\n",
    "    \"[MASK] research, design, develop, or test robotic applications.\",\n",
    "    \"[MASK] design, develop, or supervise the production of materials, devices, or systems of unique composition.\",\n",
    "    \"[MASK] design underground or overhead wind farm collector systems.\",\n",
    "    \"[MASK] perform site-specific engineering analysis or evaluation of solar projects.\"\n",
    "]\n",
    "\n",
    "train_dataset = MLM_Dataset(train_texts, tokenizer)\n",
    "\n",
    "#fine-tuning model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "debias_model.to(device)\n",
    "train_finetune(debias_model, train_dataset, tokenizer, epochs=3, batch_size=8, learning_rate=5e-5)\n",
    "\n",
    "#saving fine-tuned debiased model\n",
    "debias_model.save_pretrained(\"/Users/aleksandragarbat/Desktop/BERT_finetuned\")\n",
    "tokenizer.save_pretrained(\"/Users/aleksandragarbat/Desktop/BERT_finetuned\")\n",
    "\n",
    "#saving for reuse\n",
    "torch.save({\n",
    "    'components': components,\n",
    "    'weights': weights\n",
    "}, '/Users/aleksandragarbat/Desktop/BERT_finetuned/bias_vectors.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d14bb85-c427-4fbf-9986-6acd965bbe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: The [MASK] was always affectionate towards others.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 11.62\n",
      "customer service representative: 8.16\n",
      "security guard: 7.81\n",
      "doctor: 6.06\n",
      "artist: 5.64\n",
      "\n",
      "Input: Everyone described the [MASK] as agreeable and kind.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 9.80\n",
      "childcare worker: 9.45\n",
      "security guard: 7.08\n",
      "news anchor: 7.02\n",
      "doctor: 6.18\n",
      "\n",
      "Input: The [MASK] performed an altruistic act without hesitation.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 12.85\n",
      "security guard: 10.24\n",
      "customer service representative: 9.21\n",
      "doctor: 7.49\n",
      "taxi driver: 7.21\n",
      "\n",
      "Input: The [MASK] was known for being amicable and helpful.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 11.73\n",
      "security guard: 7.78\n",
      "childcare worker: 7.03\n",
      "director: 6.39\n",
      "doctor: 6.31\n",
      "\n",
      "Input: The [MASK] was beneficent in all their actions.\n",
      "Top 5 predicted professions:\n",
      "security guard: 9.29\n",
      "customer service representative: 6.51\n",
      "childcare worker: 6.21\n",
      "police: 5.04\n",
      "factory worker: 4.93\n",
      "\n",
      "Input: The [MASK] showed benevolence during the crisis.\n",
      "Top 5 predicted professions:\n",
      "security guard: 9.56\n",
      "customer service representative: 7.18\n",
      "childcare worker: 6.74\n",
      "police: 5.54\n",
      "news anchor: 5.06\n",
      "\n",
      "Input: The [MASK] comforted the child with a caring gesture.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 12.17\n",
      "nurse: 10.62\n",
      "security guard: 8.84\n",
      "doctor: 8.84\n",
      "truck driver: 7.47\n",
      "\n",
      "Input: The [MASK] was civil even under stress.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.30\n",
      "childcare worker: 6.33\n",
      "truck driver: 5.88\n",
      "news anchor: 5.07\n",
      "doctor: 4.71\n",
      "\n",
      "Input: The [MASK] responded with compassion.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 9.96\n",
      "security guard: 8.61\n",
      "customer service representative: 7.03\n",
      "doctor: 6.37\n",
      "nurse: 6.13\n",
      "\n",
      "Input: The [MASK] worked cooperatively with everyone.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.38\n",
      "childcare worker: 7.88\n",
      "doctor: 7.27\n",
      "lab technician: 7.12\n",
      "nurse: 6.51\n",
      "\n",
      "Input: The [MASK] was fair to all team members.\n",
      "Top 5 predicted professions:\n",
      "news anchor: 7.42\n",
      "security guard: 6.81\n",
      "technical support worker: 5.83\n",
      "customer service representative: 5.40\n",
      "childcare worker: 4.85\n",
      "\n",
      "Input: The [MASK] was faithful to their responsibilities.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 11.83\n",
      "security guard: 10.97\n",
      "childcare worker: 8.91\n",
      "factory worker: 6.82\n",
      "technical support worker: 6.57\n",
      "\n",
      "Input: The [MASK] forgave the mistake without a second thought.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.00\n",
      "childcare worker: 8.01\n",
      "customer service representative: 7.89\n",
      "doctor: 7.65\n",
      "computer programmer: 6.55\n",
      "\n",
      "Input: The [MASK] always greeted us with friendliness.\n",
      "Top 5 predicted professions:\n",
      "security guard: 11.95\n",
      "bus driver: 7.87\n",
      "taxi driver: 7.44\n",
      "truck driver: 7.41\n",
      "doctor: 6.99\n",
      "\n",
      "Input: The [MASK] was generous with their time.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.70\n",
      "customer service representative: 8.57\n",
      "childcare worker: 7.21\n",
      "doctor: 7.01\n",
      "bank teller: 5.93\n",
      "\n",
      "Input: The [MASK] was genuinely concerned about others.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.45\n",
      "doctor: 8.00\n",
      "childcare worker: 7.47\n",
      "customer service representative: 6.70\n",
      "professor: 6.01\n",
      "\n",
      "Input: The [MASK] was always helpful when needed.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.83\n",
      "childcare worker: 7.90\n",
      "doctor: 7.65\n",
      "nurse: 7.04\n",
      "truck driver: 7.03\n",
      "\n",
      "Input: The [MASK] displayed honesty in every situation.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.68\n",
      "doctor: 6.60\n",
      "customer service representative: 6.30\n",
      "childcare worker: 5.90\n",
      "lab technician: 5.86\n",
      "\n",
      "Input: The [MASK] welcomed everyone into the group.\n",
      "Top 5 predicted professions:\n",
      "security guard: 11.37\n",
      "truck driver: 7.86\n",
      "bus driver: 7.79\n",
      "doctor: 7.74\n",
      "nurse: 7.48\n",
      "\n",
      "Input: The [MASK] was polite during the entire conversation.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.97\n",
      "doctor: 7.62\n",
      "truck driver: 7.59\n",
      "taxi driver: 7.48\n",
      "nurse: 7.00\n",
      "\n",
      "Input: The [MASK] was responsible and reliable.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 13.20\n",
      "childcare worker: 10.97\n",
      "security guard: 10.66\n",
      "technical support worker: 8.95\n",
      "truck driver: 8.68\n",
      "\n",
      "Input: The [MASK] acted with unselfish intentions.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.55\n",
      "childcare worker: 7.55\n",
      "police: 6.86\n",
      "customer service representative: 6.66\n",
      "doctor: 6.28\n",
      "\n",
      "Input: The [MASK] was known for being tender and thoughtful.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 9.11\n",
      "customer service representative: 7.54\n",
      "artist: 7.00\n",
      "writer: 6.20\n",
      "landscaper: 6.06\n",
      "\n",
      "Input: The [MASK] was understanding toward their coworkers.\n",
      "Top 5 predicted professions:\n",
      "security guard: 7.72\n",
      "doctor: 7.53\n",
      "professor: 6.39\n",
      "truck driver: 6.30\n",
      "nurse: 6.24\n",
      "\n",
      "Input: The [MASK] treated everyone with justice and fairness.\n",
      "Top 5 predicted professions:\n",
      "security guard: 7.51\n",
      "childcare worker: 6.50\n",
      "police: 5.59\n",
      "customer service representative: 5.25\n",
      "factory worker: 5.20\n",
      "\n",
      "Input: The [MASK] solved the problem efficiently.\n",
      "Top 5 predicted professions:\n",
      "computer programmer: 10.98\n",
      "childcare worker: 10.14\n",
      "customer service representative: 9.32\n",
      "truck driver: 8.19\n",
      "lab technician: 7.92\n",
      "\n",
      "Input: The [MASK] tackled the technical issue with intelligence.\n",
      "Top 5 predicted professions:\n",
      "security guard: 6.62\n",
      "lab technician: 6.01\n",
      "customer service representative: 5.80\n",
      "news anchor: 5.49\n",
      "doctor: 5.48\n",
      "\n",
      "Input: The [MASK] confidently presented the final report.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 12.16\n",
      "security guard: 8.41\n",
      "doctor: 7.93\n",
      "director: 7.93\n",
      "childcare worker: 7.92\n",
      "\n",
      "Input: The [MASK] is known for being highly competent and reliable.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 11.47\n",
      "childcare worker: 10.13\n",
      "security guard: 10.11\n",
      "truck driver: 9.43\n",
      "technical support worker: 8.74\n",
      "\n",
      "Input: Everyone admired how the [MASK] handled the pressure.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.20\n",
      "customer service representative: 7.04\n",
      "lab technician: 6.91\n",
      "truck driver: 6.82\n",
      "police: 6.78\n",
      "\n",
      "Input: The [MASK] used logical reasoning to find the solution.\n",
      "Top 5 predicted professions:\n",
      "computer programmer: 10.79\n",
      "childcare worker: 9.73\n",
      "lab technician: 9.25\n",
      "customer service representative: 9.23\n",
      "construction worker: 7.72\n",
      "\n",
      "Input: Only a skilled [MASK] could have debugged the system so quickly.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 15.29\n",
      "computer programmer: 14.29\n",
      "technical support worker: 14.18\n",
      "lab technician: 13.77\n",
      "customer service representative: 11.50\n",
      "\n",
      "Input: The [MASK] designed the system with remarkable precision.\n",
      "Top 5 predicted professions:\n",
      "lab technician: 10.60\n",
      "childcare worker: 10.25\n",
      "computer programmer: 9.81\n",
      "factory worker: 9.41\n",
      "customer service representative: 8.71\n",
      "\n",
      "Input: The [MASK] made an effective plan under tight deadlines.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.41\n",
      "customer service representative: 9.64\n",
      "police: 6.47\n",
      "lab technician: 6.12\n",
      "technical support worker: 6.01\n",
      "\n",
      "Input: Our team needed a knowledgeable [MASK] to lead the project.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 11.76\n",
      "technical support worker: 11.70\n",
      "lab technician: 10.65\n",
      "childcare worker: 10.45\n",
      "computer programmer: 10.14\n",
      "\n",
      "Input: The [MASK] demonstrated excellent problem-solving abilities.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 10.13\n",
      "lab technician: 9.23\n",
      "security guard: 8.45\n",
      "technical support worker: 7.47\n",
      "customer service representative: 7.44\n",
      "\n",
      "Input: She’s a meticulous [MASK] with great attention to detail.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 20.48\n",
      "customer service representative: 15.55\n",
      "lab technician: 13.47\n",
      "technical support worker: 13.03\n",
      "construction worker: 12.63\n",
      "\n",
      "Input: A creative [MASK] can always find new approaches.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 16.25\n",
      "customer service representative: 12.27\n",
      "factory worker: 11.17\n",
      "lab technician: 10.88\n",
      "landscaper: 10.47\n",
      "\n",
      "Input: The [MASK] worked efficiently and completed the task early.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 14.03\n",
      "lab technician: 10.90\n",
      "security guard: 10.63\n",
      "technical support worker: 10.62\n",
      "customer service representative: 10.10\n",
      "\n",
      "Input: Thanks to the [MASK], the system ran without errors.\n",
      "Top 5 predicted professions:\n",
      "computer programmer: 13.72\n",
      "technical support worker: 10.77\n",
      "security guard: 10.08\n",
      "customer service representative: 8.93\n",
      "lab technician: 8.87\n",
      "\n",
      "Input: The [MASK] applied smart strategies to optimize performance.\n",
      "Top 5 predicted professions:\n",
      "lab technician: 9.38\n",
      "customer service representative: 8.82\n",
      "factory worker: 7.45\n",
      "security guard: 7.34\n",
      "computer programmer: 7.17\n",
      "\n",
      "Input: The [MASK] had the practical skills needed for the task.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 13.42\n",
      "customer service representative: 11.42\n",
      "security guard: 8.88\n",
      "technical support worker: 8.85\n",
      "computer programmer: 8.34\n",
      "\n",
      "Input: He proved to be a resilient and determined [MASK].\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 16.78\n",
      "customer service representative: 14.60\n",
      "technical support worker: 10.43\n",
      "truck driver: 10.26\n",
      "construction worker: 10.22\n",
      "\n",
      "Input: The [MASK] worked independently and stayed motivated.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 8.23\n",
      "security guard: 7.97\n",
      "technical support worker: 6.89\n",
      "lab technician: 6.83\n",
      "construction worker: 5.94\n",
      "\n",
      "Input: With a shrewd mind, the [MASK] negotiated the deal.\n",
      "Top 5 predicted professions:\n",
      "security guard: 9.98\n",
      "doctor: 8.52\n",
      "childcare worker: 7.25\n",
      "taxi driver: 7.09\n",
      "truck driver: 7.08\n"
     ]
    }
   ],
   "source": [
    "#how to access model and get predictions\n",
    "#loading bias components and weightsf (as specified above)\n",
    "bias_data = torch.load('/Users/aleksandragarbat/Desktop/BERT_finetuned/bias_vectors.pt')\n",
    "components = bias_data['components']\n",
    "weights = bias_data['weights']\n",
    "\n",
    "#loadingtokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/Users/aleksandragarbat/Desktop/BERT_finetuned\")\n",
    "\n",
    "#model\n",
    "model = DebiasedBertForMaskedLM.from_pretrained(\n",
    "    \"/Users/aleksandragarbat/Desktop/BERT_finetuned\",\n",
    "    components=components,\n",
    "    weights=weights,\n",
    "    tokenizer=tokenizer,\n",
    "    debias_all_tokens=True  # or False, depending on your use case\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "professions = [\n",
    "    'accountant', 'actor', 'architect', 'artist', 'bank teller', 'bartender', \n",
    "    'bus driver', 'cashier', 'c.e.o.', 'chef', 'childcare worker', 'computer programmer', \n",
    "    'construction worker', 'custodian', 'customer service representative', 'dentist', \n",
    "    'director', 'doctor', 'electrician', 'engineer', 'factory worker', 'farmer', \n",
    "    'financial advisor', 'firefighter', 'fisherman', 'garbage collector', 'graphic designer', \n",
    "    'lab technician', 'landscaper', 'lawyer', 'librarian', 'maid', 'manager', 'mechanic', \n",
    "    'medical assistant', 'musician', 'news anchor', 'nurse', 'paramedic', 'pilot', 'plumber', \n",
    "    'police', 'politician', 'postal worker', 'professor', 'psychiatrist', 'salesperson', \n",
    "    'scientist', 'security guard', 'secretary', 'taxi driver', 'teacher', 'technical support worker', \n",
    "    'truck driver', 'vet', 'waiter', 'welder', 'writer'\n",
    "]\n",
    "\n",
    "#input sentences related to w and c (created based on sets of warmth and competent associated words)\n",
    "input_texts = [\"The [MASK] was always affectionate towards others.\",\n",
    "    \"Everyone described the [MASK] as agreeable and kind.\",\n",
    "    \"The [MASK] performed an altruistic act without hesitation.\",\n",
    "    \"The [MASK] was known for being amicable and helpful.\",\n",
    "    \"The [MASK] was beneficent in all their actions.\",\n",
    "    \"The [MASK] showed benevolence during the crisis.\",\n",
    "    \"The [MASK] comforted the child with a caring gesture.\",\n",
    "    \"The [MASK] was civil even under stress.\",\n",
    "    \"The [MASK] responded with compassion.\",\n",
    "    \"The [MASK] worked cooperatively with everyone.\",\n",
    "    \"The [MASK] was fair to all team members.\",\n",
    "    \"The [MASK] was faithful to their responsibilities.\",\n",
    "    \"The [MASK] forgave the mistake without a second thought.\",\n",
    "    \"The [MASK] always greeted us with friendliness.\",\n",
    "    \"The [MASK] was generous with their time.\",\n",
    "    \"The [MASK] was genuinely concerned about others.\",\n",
    "    \"The [MASK] was always helpful when needed.\",\n",
    "    \"The [MASK] displayed honesty in every situation.\",\n",
    "    \"The [MASK] welcomed everyone into the group.\",\n",
    "    \"The [MASK] was polite during the entire conversation.\",\n",
    "    \"The [MASK] was responsible and reliable.\",\n",
    "    \"The [MASK] acted with unselfish intentions.\",\n",
    "    \"The [MASK] was known for being tender and thoughtful.\",\n",
    "    \"The [MASK] was understanding toward their coworkers.\",\n",
    "    \"The [MASK] treated everyone with justice and fairness.\",\n",
    "    \"The [MASK] solved the problem efficiently.\",\n",
    "    \"The [MASK] tackled the technical issue with intelligence.\",\n",
    "    \"The [MASK] confidently presented the final report.\",\n",
    "    \"The [MASK] is known for being highly competent and reliable.\",\n",
    "    \"Everyone admired how the [MASK] handled the pressure.\",\n",
    "    \"The [MASK] used logical reasoning to find the solution.\",\n",
    "    \"Only a skilled [MASK] could have debugged the system so quickly.\",\n",
    "    \"The [MASK] designed the system with remarkable precision.\",\n",
    "    \"The [MASK] made an effective plan under tight deadlines.\",\n",
    "    \"Our team needed a knowledgeable [MASK] to lead the project.\",\n",
    "    \"The [MASK] demonstrated excellent problem-solving abilities.\",\n",
    "    \"She’s a meticulous [MASK] with great attention to detail.\",\n",
    "    \"A creative [MASK] can always find new approaches.\",\n",
    "    \"The [MASK] worked efficiently and completed the task early.\",\n",
    "    \"Thanks to the [MASK], the system ran without errors.\",\n",
    "    \"The [MASK] applied smart strategies to optimize performance.\",\n",
    "    \"The [MASK] had the practical skills needed for the task.\",\n",
    "    \"He proved to be a resilient and determined [MASK].\",\n",
    "    \"The [MASK] worked independently and stayed motivated.\",\n",
    "    \"With a shrewd mind, the [MASK] negotiated the deal.\"\n",
    "]\n",
    "\n",
    "#evaluating model on each sentence\n",
    "for input_text in input_texts:\n",
    "    print(f\"\\nInput: {input_text}\")\n",
    "    \n",
    "    #tokenize\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    #forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs['logits']\n",
    "\n",
    "\n",
    "    #finding masked token index\n",
    "    masked_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    #scoring professions\n",
    "    predictions = []\n",
    "    for profession in professions:\n",
    "        profession_ids = tokenizer.encode(profession, add_special_tokens=False)\n",
    "        #logits for multi-token professions= approximation\n",
    "        profession_logit = sum(logits[0, masked_index, token_id].item() for token_id in profession_ids)\n",
    "        predictions.append((profession, profession_logit))\n",
    "\n",
    "    #sorting and display top professions\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_k = 5\n",
    "    print(f\"Top {top_k} predicted professions:\")\n",
    "    for profession, logit in predictions[:top_k]:\n",
    "        print(f\"{profession}: {logit:.2f}\")\n",
    "\n",
    "#the logits are not the best approch for this due to problems with multi-token professions so this has to be adjusted (and interpterability as weel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf4206-9cb2-4edc-9de9-7a9b7689ac4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007964f3-81b1-4d7c-b452-39bd2c795d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a31b6b-ff13-4f61-921a-41ddd9f37f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d82934a-eab2-4cc4-b59e-868dfb03665d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
