{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9581f393-b12b-4975-bf5d-6c0816aeea1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  **scheduler_specific_kwargs,\n",
      "Training Epoch 1/3: 100%|█████████████████████████| 2/2 [00:06<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11.689622402191162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/3: 100%|█████████████████████████| 2/2 [00:06<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 8.28287672996521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/3: 100%|█████████████████████████| 2/2 [00:06<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 5.846193075180054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/aleksandragarbat/Desktop/BERT_finetuned/tokenizer_config.json',\n",
       " '/Users/aleksandragarbat/Desktop/BERT_finetuned/special_tokens_map.json',\n",
       " '/Users/aleksandragarbat/Desktop/BERT_finetuned/vocab.txt',\n",
       " '/Users/aleksandragarbat/Desktop/BERT_finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finetuning + debiasing integrated\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#my vectors of w and c\n",
    "warmth_vector = pd.read_csv('/Users/aleksandragarbat/Desktop/Thesis/warmth_direction.csv').values.flatten()\n",
    "competence_vector = pd.read_csv('/Users/aleksandragarbat/Desktop/Thesis/competence_direction.csv').values.flatten()\n",
    "\n",
    "#to PyTorch tensors\n",
    "warmth_vector = torch.tensor(warmth_vector, dtype=torch.float32)\n",
    "competence_vector = torch.tensor(competence_vector, dtype=torch.float32)\n",
    "\n",
    "#normalization\n",
    "warmth_vector = warmth_vector / warmth_vector.norm()\n",
    "competence_vector = competence_vector / competence_vector.norm()\n",
    "\n",
    "#bias subspace setup\n",
    "def gram_schmidt(vectors):\n",
    "    ortho = []\n",
    "    for v in vectors:\n",
    "        for u in ortho:\n",
    "            v = v - torch.dot(v, u) * u\n",
    "        v = v / torch.norm(v)\n",
    "        ortho.append(v)\n",
    "    return ortho\n",
    "\n",
    "g0, g1 = gram_schmidt([warmth_vector, competence_vector])\n",
    "components = [g0, g1]\n",
    "weights = [0.6, 0.4]\n",
    "\n",
    "#debiasing function\n",
    "def debias_custom(h, components, weights, n_mask=None):\n",
    "    debiased = h.clone()\n",
    "    for i, g in enumerate(components):\n",
    "        weight = weights[i]\n",
    "        n = n_mask[i] if n_mask is not None else 1\n",
    "        debiased -= weight * n * torch.dot(h, g) * g\n",
    "    return debiased\n",
    "\n",
    "#dataset for fine-tuning\n",
    "class MLM_Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encoding[\"input_ids\"].flatten()\n",
    "        attention_mask = encoding[\"attention_mask\"].flatten()\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids  #labels= input_ids\n",
    "        }\n",
    "\n",
    "#f-t loop with debiasing\n",
    "def train_finetune(model, train_dataset, tokenizer, epochs=3, batch_size=8, learning_rate=5e-5):\n",
    "    model.train()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            labels = batch[\"labels\"].to(model.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with debiasing logic\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']  #accessing loss as dictionary key\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_loader)}\")\n",
    "\n",
    "#extension of BERT with debiasing lkogic application\n",
    "class DebiasedBertForMaskedLM(BertForMaskedLM):\n",
    "    def __init__(self, config, components, weights, tokenizer):\n",
    "        super().__init__(config)\n",
    "        self.components = components\n",
    "        self.weights = weights\n",
    "        self.tokenizer = tokenizer  #tokenizer as attribute\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        #forward pass through BERT\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        #debiasing all [MASK] token rep. in batch\n",
    "        mask_token_index = (input_ids == self.tokenizer.mask_token_id)  # Use tokenizer for mask_token_id\n",
    "        debiased_sequence_output = sequence_output.clone()\n",
    "\n",
    "        for batch_idx in range(input_ids.size(0)):\n",
    "            for token_idx in torch.where(mask_token_index[batch_idx])[0]:\n",
    "                h = sequence_output[batch_idx, token_idx]\n",
    "                debiased_h = debias_custom(h, self.components, self.weights)\n",
    "                debiased_sequence_output[batch_idx, token_idx] = debiased_h\n",
    "\n",
    "        #forward through LM head\n",
    "        prediction_scores = self.cls(debiased_sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": prediction_scores}  # Ensure loss is always returned as a part of the dictionary\n",
    "\n",
    "\n",
    "#loading tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#initializing model\n",
    "debias_model = DebiasedBertForMaskedLM.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    components=components, \n",
    "    weights=weights,\n",
    "    tokenizer=tokenizer  # Pass tokenizer explicitly here\n",
    ")\n",
    "\n",
    "\n",
    "#dataset for f-t (should be added more data- i took this from O'net job descriotions)\n",
    "train_texts = [\n",
    "    \"[MASK] conduct subsurface surveys to identify the characteristics of potential land or mining development sites. May specify the ground support systems, processes, and equipment for safe, economical, and environmentally sound extraction or underground construction activities. May inspect areas for unsafe geological conditions, equipment, and working conditions. May design, implement, and coordinate mine safety programs.\",\n",
    "    \"[MASK] conduct research on nuclear engineering projects or apply principles and theory of nuclear science to problems concerned with release, control, and use of nuclear energy and nuclear waste disposal.\",\n",
    "    \"[MASK] devise methods to improve oil and gas extraction and production and determine the need for new or modified tool designs. Oversee drilling and offer technical advice.\",\n",
    "    \"[MASK] all engineers not listed separately.\",\n",
    "    \"[MASK] design, develop, or evaluate energy-related projects or programs to reduce energy costs or improve energy efficiency during the designing, building, or remodeling stages of construction. May specialize in electrical systems; heating, ventilation, and air-conditioning (HVAC) systems; green buildings; lighting; air quality; or energy procurement.\",\n",
    "    \"[MASK] research, design, develop, or test automation, intelligent systems, smart devices, or industrial systems control.\",\n",
    "    \"[MASK] research, design, develop, or test microelectromechanical systems (MEMS) devices.\",\n",
    "    \"[MASK] design technologies specializing in light information or light energy, such as laser or fiber optics technology.\",\n",
    "    \"[MASK] research, design, develop, or test robotic applications.\",\n",
    "    \"[MASK] design, develop, or supervise the production of materials, devices, or systems of unique molecular or macromolecular composition, applying principles of nanoscale physics and electrical, chemical, or biological engineering.\",\n",
    "    \"[MASK] design underground or overhead wind farm collector systems and prepare and develop site specifications.\",\n",
    "    \"[MASK] perform site-specific engineering analysis or evaluation of energy efficiency and solar projects involving residential, commercial, or industrial customers. Design solar domestic hot water and space heating systems for new and existing structures, applying knowledge of structural energy requirements, local climates, solar technology, and thermodynamics.\"\n",
    "]\n",
    "\n",
    "train_dataset = MLM_Dataset(train_texts, tokenizer)\n",
    "\n",
    "\n",
    "#fine tune model\n",
    "train_finetune(debias_model, train_dataset, tokenizer, epochs=3, batch_size=8, learning_rate=5e-5)\n",
    "\n",
    "#save fine-tuned debiased model\n",
    "debias_model.save_pretrained(\"/Users/aleksandragarbat/Desktop/BERT_finetuned\")\n",
    "tokenizer.save_pretrained(\"/Users/aleksandragarbat/Desktop/BERT_finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0d14bb85-c427-4fbf-9986-6acd965bbe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: The [MASK] was always affectionate towards others.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 9.999423146247864\n",
      "customer service representative: 7.039122700691223\n",
      "security guard: 6.462022304534912\n",
      "doctor: 5.498518466949463\n",
      "teacher: 5.397565841674805\n",
      "\n",
      "Input: Everyone described the [MASK] as agreeable and kind.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 8.99599289894104\n",
      "childcare worker: 7.8767805099487305\n",
      "news anchor: 6.718981742858887\n",
      "security guard: 5.610380411148071\n",
      "doctor: 5.557197570800781\n",
      "\n",
      "Input: The [MASK] performed an altruistic act without hesitation.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 10.607293963432312\n",
      "security guard: 8.749783873558044\n",
      "customer service representative: 8.258424580097198\n",
      "doctor: 6.820766925811768\n",
      "taxi driver: 6.639878630638123\n",
      "\n",
      "Input: The [MASK] was known for being amicable and helpful.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 10.905338287353516\n",
      "director: 6.229388236999512\n",
      "security guard: 6.133108019828796\n",
      "childcare worker: 5.991461515426636\n",
      "writer: 5.966535568237305\n",
      "\n",
      "Input: The [MASK] was beneficent in all their actions.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.133716583251953\n",
      "customer service representative: 5.907182574272156\n",
      "childcare worker: 5.109172582626343\n",
      "police: 4.9137725830078125\n",
      "factory worker: 4.465394735336304\n",
      "\n",
      "Input: The [MASK] showed benevolence during the crisis.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.292771577835083\n",
      "customer service representative: 6.934324026107788\n",
      "childcare worker: 5.945954084396362\n",
      "police: 5.268847942352295\n",
      "factory worker: 4.850850582122803\n",
      "\n",
      "Input: The [MASK] comforted the child with a caring gesture.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 10.866517782211304\n",
      "nurse: 9.939868927001953\n",
      "doctor: 8.333996772766113\n",
      "security guard: 7.780007839202881\n",
      "truck driver: 7.403827905654907\n",
      "\n",
      "Input: The [MASK] was civil even under stress.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.806106805801392\n",
      "truck driver: 5.734220504760742\n",
      "childcare worker: 5.397847294807434\n",
      "news anchor: 4.553222298622131\n",
      "doctor: 4.324256896972656\n",
      "\n",
      "Input: The [MASK] responded with compassion.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 8.467384457588196\n",
      "security guard: 7.4067622423172\n",
      "customer service representative: 6.430699348449707\n",
      "doctor: 5.834518909454346\n",
      "nurse: 5.4052815437316895\n",
      "\n",
      "Input: The [MASK] worked cooperatively with everyone.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.991221904754639\n",
      "doctor: 6.693935394287109\n",
      "childcare worker: 6.57411253452301\n",
      "lab technician: 6.1435370445251465\n",
      "truck driver: 5.992339134216309\n",
      "\n",
      "Input: The [MASK] was fair to all team members.\n",
      "Top 5 predicted professions:\n",
      "news anchor: 7.043706893920898\n",
      "security guard: 5.203985929489136\n",
      "customer service representative: 4.7480484545230865\n",
      "technical support worker: 4.7360318303108215\n",
      "childcare worker: 3.57424695789814\n",
      "\n",
      "Input: The [MASK] was faithful to their responsibilities.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 10.7568838596344\n",
      "security guard: 9.306416988372803\n",
      "childcare worker: 7.602117657661438\n",
      "factory worker: 6.292396068572998\n",
      "construction worker: 5.951685428619385\n",
      "\n",
      "Input: The [MASK] forgave the mistake without a second thought.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.510154247283936\n",
      "customer service representative: 7.368343114852905\n",
      "doctor: 6.9087815284729\n",
      "childcare worker: 6.616665601730347\n",
      "lawyer: 6.217092514038086\n",
      "\n",
      "Input: The [MASK] always greeted us with friendliness.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.55615758895874\n",
      "bus driver: 7.703146696090698\n",
      "truck driver: 7.46633505821228\n",
      "taxi driver: 7.143403768539429\n",
      "doctor: 6.540900707244873\n",
      "\n",
      "Input: The [MASK] was generous with their time.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 8.075807809829712\n",
      "security guard: 7.303053617477417\n",
      "doctor: 6.511403560638428\n",
      "childcare worker: 6.322357535362244\n",
      "bank teller: 5.877825856208801\n",
      "\n",
      "Input: The [MASK] was genuinely concerned about others.\n",
      "Top 5 predicted professions:\n",
      "doctor: 7.4425787925720215\n",
      "security guard: 7.197041749954224\n",
      "customer service representative: 6.241009682416916\n",
      "childcare worker: 6.157036781311035\n",
      "professor: 6.063976764678955\n",
      "\n",
      "Input: The [MASK] was always helpful when needed.\n",
      "Top 5 predicted professions:\n",
      "security guard: 7.634070992469788\n",
      "doctor: 7.091651439666748\n",
      "truck driver: 6.977896451950073\n",
      "childcare worker: 6.971987962722778\n",
      "nurse: 6.4608845710754395\n",
      "\n",
      "Input: The [MASK] displayed honesty in every situation.\n",
      "Top 5 predicted professions:\n",
      "security guard: 7.328718066215515\n",
      "customer service representative: 6.1857463121414185\n",
      "doctor: 6.144814968109131\n",
      "professor: 5.242300510406494\n",
      "lab technician: 5.147090435028076\n",
      "\n",
      "Input: The [MASK] welcomed everyone into the group.\n",
      "Top 5 predicted professions:\n",
      "security guard: 10.218736410140991\n",
      "truck driver: 7.7360734939575195\n",
      "bus driver: 7.466309070587158\n",
      "doctor: 7.203976631164551\n",
      "nurse: 6.8750505447387695\n",
      "\n",
      "Input: The [MASK] was polite during the entire conversation.\n",
      "Top 5 predicted professions:\n",
      "security guard: 9.611167669296265\n",
      "truck driver: 7.375675201416016\n",
      "doctor: 7.078192234039307\n",
      "taxi driver: 6.975483179092407\n",
      "professor: 6.441460609436035\n",
      "\n",
      "Input: The [MASK] was responsible and reliable.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 11.665531635284424\n",
      "childcare worker: 9.315653920173645\n",
      "security guard: 8.887381315231323\n",
      "truck driver: 8.318893194198608\n",
      "technical support worker: 7.569302275776863\n",
      "\n",
      "Input: The [MASK] acted with unselfish intentions.\n",
      "Top 5 predicted professions:\n",
      "security guard: 9.085010051727295\n",
      "police: 6.333146572113037\n",
      "customer service representative: 6.004122853279114\n",
      "childcare worker: 5.9892897605896\n",
      "doctor: 5.592752933502197\n",
      "\n",
      "Input: The [MASK] was known for being tender and thoughtful.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 7.84660530090332\n",
      "artist: 6.989295482635498\n",
      "customer service representative: 6.950678825378418\n",
      "writer: 6.459261417388916\n",
      "actor: 5.519405841827393\n",
      "\n",
      "Input: The [MASK] was understanding toward their coworkers.\n",
      "Top 5 predicted professions:\n",
      "doctor: 6.899418354034424\n",
      "security guard: 6.481473326683044\n",
      "professor: 6.450934410095215\n",
      "truck driver: 6.312702417373657\n",
      "director: 5.842766284942627\n",
      "\n",
      "Input: The [MASK] treated everyone with justice and fairness.\n",
      "Top 5 predicted professions:\n",
      "security guard: 6.471651077270508\n",
      "childcare worker: 5.498863577842712\n",
      "police: 5.283543109893799\n",
      "customer service representative: 4.92837293446064\n",
      "factory worker: 4.867273807525635\n",
      "\n",
      "Input: The [MASK] solved the problem efficiently.\n",
      "Top 5 predicted professions:\n",
      "computer programmer: 9.776822090148926\n",
      "childcare worker: 8.713772773742676\n",
      "customer service representative: 8.346201181411743\n",
      "truck driver: 7.815740585327148\n",
      "lab technician: 7.256169080734253\n",
      "\n",
      "Input: The [MASK] tackled the technical issue with intelligence.\n",
      "Top 5 predicted professions:\n",
      "news anchor: 5.468350887298584\n",
      "lab technician: 5.444192886352539\n",
      "pilot: 5.299319744110107\n",
      "customer service representative: 5.241180658340454\n",
      "doctor: 5.128773212432861\n",
      "\n",
      "Input: The [MASK] confidently presented the final report.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 11.030625104904175\n",
      "director: 7.7394022941589355\n",
      "lawyer: 7.432602405548096\n",
      "doctor: 7.350049018859863\n",
      "secretary: 7.262480735778809\n",
      "\n",
      "Input: The [MASK] is known for being highly competent and reliable.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 10.902490139007568\n",
      "childcare worker: 9.177321922034025\n",
      "truck driver: 8.88392162322998\n",
      "security guard: 8.608649253845215\n",
      "bus driver: 7.860796809196472\n",
      "\n",
      "Input: Everyone admired how the [MASK] handled the pressure.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.826331377029419\n",
      "truck driver: 6.785311460494995\n",
      "bus driver: 6.466631889343262\n",
      "customer service representative: 6.318003416061401\n",
      "doctor: 6.275392055511475\n",
      "\n",
      "Input: The [MASK] used logical reasoning to find the solution.\n",
      "Top 5 predicted professions:\n",
      "computer programmer: 9.668398380279541\n",
      "customer service representative: 8.307524681091309\n",
      "lab technician: 8.28818678855896\n",
      "childcare worker: 7.844305694103241\n",
      "scientist: 7.076050758361816\n",
      "\n",
      "Input: Only a skilled [MASK] could have debugged the system so quickly.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 14.073423959314823\n",
      "technical support worker: 13.55121386051178\n",
      "computer programmer: 13.407285213470459\n",
      "lab technician: 12.782754898071289\n",
      "customer service representative: 11.224447011947632\n",
      "\n",
      "Input: The [MASK] designed the system with remarkable precision.\n",
      "Top 5 predicted professions:\n",
      "lab technician: 10.350653171539307\n",
      "computer programmer: 9.442036628723145\n",
      "factory worker: 9.283047199249268\n",
      "childcare worker: 9.261882185935974\n",
      "engineer: 8.447982788085938\n",
      "\n",
      "Input: The [MASK] made an effective plan under tight deadlines.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.784161567687988\n",
      "customer service representative: 8.754998445510864\n",
      "police: 6.025379180908203\n",
      "director: 5.557593822479248\n",
      "lab technician: 5.394961357116699\n",
      "\n",
      "Input: Our team needed a knowledgeable [MASK] to lead the project.\n",
      "Top 5 predicted professions:\n",
      "customer service representative: 10.944560706615448\n",
      "technical support worker: 10.784864783287048\n",
      "lab technician: 9.86069655418396\n",
      "computer programmer: 9.633825302124023\n",
      "childcare worker: 9.297443151473999\n",
      "\n",
      "Input: The [MASK] demonstrated excellent problem-solving abilities.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 8.645159363746643\n",
      "lab technician: 8.42214059829712\n",
      "customer service representative: 7.113116264343262\n",
      "security guard: 6.994361519813538\n",
      "truck driver: 6.425061345100403\n",
      "\n",
      "Input: She’s a meticulous [MASK] with great attention to detail.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 19.66651487350464\n",
      "customer service representative: 15.266925573348999\n",
      "lab technician: 12.7358078956604\n",
      "technical support worker: 12.69971251487732\n",
      "construction worker: 12.47651195526123\n",
      "\n",
      "Input: A creative [MASK] can always find new approaches.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 14.604008376598358\n",
      "customer service representative: 11.48862910270691\n",
      "factory worker: 10.29290246963501\n",
      "lab technician: 9.827388286590576\n",
      "landscaper: 9.0381441116333\n",
      "\n",
      "Input: The [MASK] worked efficiently and completed the task early.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 11.476181771606207\n",
      "lab technician: 9.680693864822388\n",
      "technical support worker: 9.196318626403809\n",
      "security guard: 9.054501295089722\n",
      "customer service representative: 9.031028747558594\n",
      "\n",
      "Input: Thanks to the [MASK], the system ran without errors.\n",
      "Top 5 predicted professions:\n",
      "computer programmer: 13.156297206878662\n",
      "technical support worker: 10.332754373550415\n",
      "security guard: 8.912988662719727\n",
      "customer service representative: 8.832798600196838\n",
      "truck driver: 8.620484590530396\n",
      "\n",
      "Input: The [MASK] applied smart strategies to optimize performance.\n",
      "Top 5 predicted professions:\n",
      "lab technician: 9.055084705352783\n",
      "customer service representative: 8.541469812393188\n",
      "factory worker: 7.681126117706299\n",
      "construction worker: 6.869540691375732\n",
      "computer programmer: 6.4848902225494385\n",
      "\n",
      "Input: The [MASK] had the practical skills needed for the task.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 11.245237946510315\n",
      "customer service representative: 9.858205556869507\n",
      "security guard: 7.203663110733032\n",
      "computer programmer: 7.1132493019104\n",
      "lab technician: 6.9575865268707275\n",
      "\n",
      "Input: He proved to be a resilient and determined [MASK].\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 15.566521525382996\n",
      "customer service representative: 14.218740582466125\n",
      "truck driver: 10.1364107131958\n",
      "technical support worker: 9.741568922996521\n",
      "construction worker: 9.669052362442017\n",
      "\n",
      "Input: The [MASK] worked independently and stayed motivated.\n",
      "Top 5 predicted professions:\n",
      "childcare worker: 6.291500926017761\n",
      "security guard: 6.1651328802108765\n",
      "lab technician: 5.609267711639404\n",
      "technical support worker: 5.508672431111336\n",
      "construction worker: 5.086044549942017\n",
      "\n",
      "Input: With a shrewd mind, the [MASK] negotiated the deal.\n",
      "Top 5 predicted professions:\n",
      "security guard: 8.662086963653564\n",
      "doctor: 7.842931270599365\n",
      "lawyer: 6.977907180786133\n",
      "truck driver: 6.844704031944275\n",
      "taxi driver: 6.499358415603638\n"
     ]
    }
   ],
   "source": [
    "#how to access model and get predictions\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/Users/aleksandragarbat/Desktop/BERT_finetuned\")\n",
    "model = BertForMaskedLM.from_pretrained(\"/Users/aleksandragarbat/Desktop/BERT_finetuned\")\n",
    "\n",
    "#list of professions to predict from\n",
    "professions = [\n",
    "    'accountant', 'actor', 'architect', 'artist', 'bank teller', 'bartender', \n",
    "    'bus driver', 'cashier', 'c.e.o.', 'chef', 'childcare worker', 'computer programmer', \n",
    "    'construction worker', 'custodian', 'customer service representative', 'dentist', \n",
    "    'director', 'doctor', 'electrician', 'engineer', 'factory worker', 'farmer', \n",
    "    'financial advisor', 'firefighter', 'fisherman', 'garbage collector', 'graphic designer', \n",
    "    'lab technician', 'landscaper', 'lawyer', 'librarian', 'maid', 'manager', 'mechanic', \n",
    "    'medical assistant', 'musician', 'news anchor', 'nurse', 'paramedic', 'pilot', 'plumber', \n",
    "    'police', 'politician', 'postal worker', 'professor', 'psychiatrist', 'salesperson', \n",
    "    'scientist', 'security guard', 'secretary', 'taxi driver', 'teacher', 'technical support worker', \n",
    "    'truck driver', 'vet', 'waiter', 'welder', 'writer'\n",
    "]\n",
    "\n",
    "# sentences with masked token (created based on sets of warmth and competent associated words)\n",
    "input_texts = [\"The [MASK] was always affectionate towards others.\",\n",
    "    \"Everyone described the [MASK] as agreeable and kind.\",\n",
    "    \"The [MASK] performed an altruistic act without hesitation.\",\n",
    "    \"The [MASK] was known for being amicable and helpful.\",\n",
    "    \"The [MASK] was beneficent in all their actions.\",\n",
    "    \"The [MASK] showed benevolence during the crisis.\",\n",
    "    \"The [MASK] comforted the child with a caring gesture.\",\n",
    "    \"The [MASK] was civil even under stress.\",\n",
    "    \"The [MASK] responded with compassion.\",\n",
    "    \"The [MASK] worked cooperatively with everyone.\",\n",
    "    \"The [MASK] was fair to all team members.\",\n",
    "    \"The [MASK] was faithful to their responsibilities.\",\n",
    "    \"The [MASK] forgave the mistake without a second thought.\",\n",
    "    \"The [MASK] always greeted us with friendliness.\",\n",
    "    \"The [MASK] was generous with their time.\",\n",
    "    \"The [MASK] was genuinely concerned about others.\",\n",
    "    \"The [MASK] was always helpful when needed.\",\n",
    "    \"The [MASK] displayed honesty in every situation.\",\n",
    "    \"The [MASK] welcomed everyone into the group.\",\n",
    "    \"The [MASK] was polite during the entire conversation.\",\n",
    "    \"The [MASK] was responsible and reliable.\",\n",
    "    \"The [MASK] acted with unselfish intentions.\",\n",
    "    \"The [MASK] was known for being tender and thoughtful.\",\n",
    "    \"The [MASK] was understanding toward their coworkers.\",\n",
    "    \"The [MASK] treated everyone with justice and fairness.\",\n",
    "    \"The [MASK] solved the problem efficiently.\",\n",
    "    \"The [MASK] tackled the technical issue with intelligence.\",\n",
    "    \"The [MASK] confidently presented the final report.\",\n",
    "    \"The [MASK] is known for being highly competent and reliable.\",\n",
    "    \"Everyone admired how the [MASK] handled the pressure.\",\n",
    "    \"The [MASK] used logical reasoning to find the solution.\",\n",
    "    \"Only a skilled [MASK] could have debugged the system so quickly.\",\n",
    "    \"The [MASK] designed the system with remarkable precision.\",\n",
    "    \"The [MASK] made an effective plan under tight deadlines.\",\n",
    "    \"Our team needed a knowledgeable [MASK] to lead the project.\",\n",
    "    \"The [MASK] demonstrated excellent problem-solving abilities.\",\n",
    "    \"She’s a meticulous [MASK] with great attention to detail.\",\n",
    "    \"A creative [MASK] can always find new approaches.\",\n",
    "    \"The [MASK] worked efficiently and completed the task early.\",\n",
    "    \"Thanks to the [MASK], the system ran without errors.\",\n",
    "    \"The [MASK] applied smart strategies to optimize performance.\",\n",
    "    \"The [MASK] had the practical skills needed for the task.\",\n",
    "    \"He proved to be a resilient and determined [MASK].\",\n",
    "    \"The [MASK] worked independently and stayed motivated.\",\n",
    "    \"With a shrewd mind, the [MASK] negotiated the deal.\"\n",
    "]\n",
    "\n",
    "\n",
    "#process each specified above input text\n",
    "for input_text in input_texts:\n",
    "    print(f\"\\nInput: {input_text}\")\n",
    "    \n",
    "    #tokenize input\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the logits from the model\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    #obtaining predicted token ids for masked position\n",
    "    masked_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    #for each profession predict likelihood of it being masked token\n",
    "    predictions = []\n",
    "    for profession in professions:\n",
    "        # Tokenize the profession and get its token id\n",
    "        profession_ids = tokenizer.encode(profession, add_special_tokens=False)\n",
    "        \n",
    "        #for each token in profession finding its predicted logit value\n",
    "        profession_logit = 0\n",
    "        for token_id in profession_ids:\n",
    "            # Get the logit for the token at the masked position\n",
    "            profession_logit += logits[0, masked_index, token_id].item()\n",
    "\n",
    "        predictions.append((profession, profession_logit))\n",
    "\n",
    "    #sorting the predictions by logit value (highest probability for masked token)\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    #how many professions it will specify(k)\n",
    "    k = 5\n",
    "\n",
    "    #most likely predictions \n",
    "    top_k_professions = predictions[:k]\n",
    "\n",
    "    #printitng top k professions for givrn input sentences\n",
    "    print(f\"Top {k} predicted professions:\")\n",
    "    for profession, logit in top_k_professions:\n",
    "        print(f\"{profession}: {logit}\")\n",
    "\n",
    "\n",
    "#larger logit larger prob- these can aslo be converted to prob (eg profession x is the most associated with masked token)\n",
    "#the multi token professions needs to be verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf4206-9cb2-4edc-9de9-7a9b7689ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007964f3-81b1-4d7c-b452-39bd2c795d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
